# Commonly Used Models in Fraud Detection

1. **Logistic Regression**
    - **Predicted Fraud Rate:**  
      \( \hat{y} = \sigma(\mathbf{w}^T \mathbf{x} + b) = \frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{x} + b)}} \)  
      where \(\mathbf{x}\) = feature vector, \(\mathbf{w}\) = weights, \(b\) = bias, \(\sigma\) = sigmoid function.
    - **Optimized Function (Binary Cross-Entropy Loss):**  
      \( L = -\frac{1}{N} \sum_{i=1}^N [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)] \)

2. **Random Forest**
    - **Predicted Fraud Rate:**  
      \( \hat{y} = \frac{1}{T} \sum_{t=1}^T h_t(\mathbf{x}) \)  
      where \(h_t\) = prediction of tree \(t\), \(T\) = number of trees.
    - **Optimized Function:**  
      Each tree minimizes impurity (e.g., Gini impurity or entropy) at each split:
      - Gini: \( G = 1 - \sum_{k=1}^K p_k^2 \)
      - Entropy: \( H = -\sum_{k=1}^K p_k \log(p_k) \)
      where \(p_k\) = proportion of class \(k\) in a node.

3. **Gradient Boosted Trees (e.g., XGBoost, LightGBM)**
    - **Predicted Fraud Rate:**  
      \( \hat{y} = \sigma\left(\sum_{m=1}^M f_m(\mathbf{x})\right) \)  
      where \(f_m\) = prediction from tree \(m\), \(M\) = number of trees.
    - **Optimized Function:**  
      Minimizes differentiable loss (e.g., binary cross-entropy) with regularization:
      \( L = \sum_{i=1}^N l(y_i, \hat{y}_i) + \sum_{m=1}^M \Omega(f_m) \)
      where \(\Omega\) = regularization term.

4. **Neural Networks**
    - **Predicted Fraud Rate:**  
      \( \hat{y} = \sigma(\mathbf{W}_2 \cdot \text{ReLU}(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1) + b_2) \)
    - **Optimized Function:**  
      Binary cross-entropy loss (same as logistic regression).

5. **Naive Bayes**
    - **Predicted Fraud Rate:**  
      \( P(y=1|\mathbf{x}) = \frac{P(\mathbf{x}|y=1)P(y=1)}{P(\mathbf{x})} \)
      - For Gaussian NB:  
         \( P(x_j|y) = \frac{1}{\sqrt{2\pi\sigma_{y,j}^2}} \exp\left(-\frac{(x_j - \mu_{y,j})^2}{2\sigma_{y,j}^2}\right) \)
    - **Optimized Function:**  
      Maximizes likelihood (MLE) of parameters given the data.

# Notes
- \(\mathbf{x}\): Feature vector for a transaction.
- \(y\): True label (1 = fraud, 0 = not fraud).
- \(\hat{y}\): Predicted probability of fraud.
- All models are typically trained to minimize a loss function (e.g., cross-entropy) over the training data.